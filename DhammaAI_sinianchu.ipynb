{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c4069a4-5317-4830-8c1b-a21e53c1ce65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ChatVectorDBChain\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "import jieba as jb\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e72ac78d-9468-47ff-bb4f-194acb22f1b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.022 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "files=['Dhammatest.txt','merged.txt']\n",
    "#files=['四念住.txt']\n",
    " \n",
    "for file in files:\n",
    "    #读取data文件夹中的中文文档\n",
    "    my_file=f\"./data/{file}\"\n",
    "    with open(my_file,\"r\",encoding='utf-8') as f:  \n",
    "        data = f.read()\n",
    "    \n",
    "    #对中文文档进行分词处理\n",
    "    cut_data = \" \".join([w for w in list(jb.cut(data))])\n",
    "    #分词处理后的文档保存到data文件夹中的cut子文件夹中\n",
    "    cut_file=f\"./data/cut/cut_{file}\"\n",
    "    with open(cut_file, 'w') as f:   \n",
    "        f.write(cut_data)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54ae087b-d3fa-4167-a0e1-02a9432c52b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "Using embedded DuckDB with persistence: data will be stored in: ./data/cut\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d5fdc39a7546eabeee8bbbb513cf7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='100%'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/langchain/llms/openai.py:169: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/langchain/llms/openai.py:623: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py:191: UserWarning: `ChatVectorDBChain` is deprecated - please use `from langchain.chains import ConversationalRetrievalChain`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#加载文档\n",
    "loader = DirectoryLoader('./data/cut',glob='**/*.txt')\n",
    "docs = loader.load()\n",
    "#文档切块\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "doc_texts = text_splitter.split_documents(docs)\n",
    "#调用openai Embeddings\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "#向量化\n",
    "vectordb = Chroma.from_documents(doc_texts, embeddings, persist_directory=\"./data/cut\")\n",
    "vectordb.persist()\n",
    "#创建聊天机器人对象chain,并且返回引用了哪个文档\n",
    "chain = ChatVectorDBChain.from_llm(OpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"), vectordb, return_source_documents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "992be675-d659-43d9-9991-b1bbda14711f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_answer(question):\n",
    "  chat_history = []\n",
    "  result = chain({\"question\": question, \"chat_history\": chat_history});\n",
    "  return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "846cc207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这篇文章中提到了中国人的反应会慢一点，因为需要等翻译。同时也提到了在泰国，中国人的数量约为500万人。但是文章并没有详细描述中国人的整体状况。\n"
     ]
    }
   ],
   "source": [
    "question='中国人的状况怎么样?用中文回复'\n",
    "print(get_answer(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0342fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context does not provide any information about what \"涅槃\" means.\n"
     ]
    }
   ],
   "source": [
    "question='涅槃是什么?用中文回复'\n",
    "print(get_answer(question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
